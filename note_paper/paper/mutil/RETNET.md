# Retentive Network: A Successor to Transformer for Large Language Models

记忆网络：大型语言模型中 Transformer 的继任者

## 摘要

在这项工作中，我们提出保留网络（RETNET）作为大型语言模型的基础架构，它能同时实现训练并行性、低成本推理以及良好的性能表现。我们从理论上推导了循环（recurrence）和注意力（attention）之间的联系。随后，我们提出了用于序列建模的保留机制，该机制支持三种计算范式，即并行式、循环式以及分块循环式。 具体而言，并行式表示支持训练并行化；循环式表示能够实现低成本的O(1)复杂度推理，这在不牺牲性能的情况下，提高了解码吞吐量、降低了延迟并减少了GPU内存占用；分块循环式表示有助于以线性复杂度进行高效的长序列建模，在这种方式下，每个分块可并行编码，同时对各分块进行循环式汇总。 在语言建模方面的实验结果表明，保留网络（RETNET）实现了良好的规模化效果、并行训练、低成本部署以及高效推理。这些引人注目的特性使得保留网络（RETNET）成为大型语言模型领域中Transformer强有力的继任者。代码将发布在https://aka.ms/retnet网站上。 

## 一、介绍

![image-20241121111901294](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211119419.png)

> 图1：与Transformer相比，保留网络（RetNet）实现了低成本推理（即GPU内存占用更少、吞吐量更高、延迟更低）、训练并行化以及良好的缩放曲线。推理成本的相关结果是在输入长度为8000的情况下报告的。图6展示了在不同序列长度下的更多结果。

在这项工作中，我们提出了保留网络（RetNet），它能同时实现低成本推理、高效的长序列建模、可与Transformer相媲美的性能以及并行模型训练。 具体而言，我们引入了一种多尺度记忆机制来替代多头注意力机制，该机制具备三种计算范式，即并行表示、循环表示以及分块循环表示。 首先，并行表示赋予了训练并行化的能力，能够充分利用GPU设备。其次，循环表示使得在内存和计算方面能够实现高效的O(1)复杂度推理。部署成本和延迟可显著降低，而且在无需键值缓存技巧的情况下，其实现过程也大大简化了。再者，分块循环表示可以执行高效的长序列建模，我们对每个局部块进行并行编码以提高计算速度，同时对全局块进行循环编码以节省GPU内存。 

我们开展了大量实验，将保留网络（RetNet）与Transformer及其变体进行对比。语言建模方面的实验结果表明，保留网络（RetNet）在缩放曲线和情境学习方面始终具有竞争力。此外，保留网络（RetNet）的推理成本与序列长度无关。 对于一个70亿参数的模型以及长度为8000的序列，与使用键值缓存的Transformer相比，保留网络（RetNet）的解码速度快8.4倍，并且能节省70%的内存。在训练期间，与标准Transformer相比，保留网络（RetNet）还能节省25% - 50%的内存，并实现7倍的加速，相较于高度优化的FlashAttention [DFE + 22]也具有优势。此外，保留网络（RetNet）的推理延迟对批量大小不敏感，这使得它能实现极大的吞吐量。这些引人注目的特性使得保留网络（RetNet）成为大型语言模型领域中Transformer强有力的继任者。 



## 二、记忆网络

记忆网络（RetNet）由 $L$ 个相同的模块堆叠而成，其布局（即残差连接和前置层归一化）与Transformer 类似。每个记忆网络（RetNet）模块包含两个子模块：一个多尺度记忆（MSR）模块和一个前馈网络（FFN）模块。 我们将在接下来的章节中介绍多尺度记忆（MSR）模块。给定一个输入序列  $x = x_{1}\cdots x_{|x|}$，保留网络（RetNet）以自回归的方式对该序列进行编码。首先，将输入向量 $\{x_{i}\}_{i = 1}^{|x|}$ 打包成 $X_{0} = [x_{1}, \cdots, x_{|x|}] \in \mathbb{R}^{|x| \times d_{model}}$ ，其中$d_{model}$ 是隐藏层维度。然后，我们通过计算得到带有上下文信息的向量表示 $X_{l} = \text{RetNet}_{l}(X_{l - 1})$ ，其中 $ l\in [1, L]$。 

### 2.1 Retention

在本节中，我们将介绍具有循环和并行双重形式的记忆力机制。这样一来，我们就能以并行的方式训练模型，同时以循环的方式进行推理。 给定输入 $X \in \mathbb{R}^{|x| \times d_{model}}$，我们将其投影到一维函数 $v(n) = X_{n} \cdot w_{V}$ 上。考虑一个通过状态 $s_{n}$ 将 $v(n)$ 映射为 $o(n)$ 的序列建模问题。为简便起见，令 $v_{n}$、$o_{n}$ 分别表示 $v(n)$ 和 $o(n)$。我们以循环的方式来构建这个映射关系： 

![image-20241121160833957](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211608012.png)

在这个过程中，我们将 $v_{n}$ 映射到状态向量 $s_{n}$，然后实施一个线性变换，以循环的方式对序列信息进行编码。 

接下来，我们使投影 $Q_{n}$、 $K_{n}$ 具备内容感知能力：

![image-20241121161226561](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211612654.png)

其中，$W_{Q}$、$W_{K} \in \mathbb{R}^{d\times d}$ 是可学习的矩阵。 

我们对矩阵 $A = \Lambda(\gamma e^{i\theta})\Lambda^{-1}$ 进行对角化处理，这里 $\gamma$、$\theta \in \mathbb{R}^{d}$ 。

> ### 分解的含义
>
> 1. **形式解析：**
>
>    $γeiθ=γ(cos⁡θ+isin⁡θ)$,
>
>    其中 $γ∈Rd$ 是模长，$\theta \in \mathbb{R}^d$ 是对应的相位角（复数的辐角）。
>
> 2. 这是一个典型的复数形式的对角矩阵，其对角元为复数 $\gamma_k e^{i\theta_k}$，表示每个特征值以极坐标形式给出。这意味着矩阵 A 的特征值可能是复数，而特征值决定了矩阵的性质。
>
> 3. **复数的来源：**
>    实数矩阵 A 的特征值不一定是实数；它可以是复数，但必须成对出现（如果 A 是实矩阵）。复数特征值的形式 $\gamma e^{i\theta}$ 是复数的极坐标表示，因而自然可能包含虚部。

然后我们得到 $A^{n - m} = \Lambda(\gamma e^{i\theta})^{n - m}\Lambda^{-1}$。通过将 $\Lambda$ 吸收进 $W_{Q}$ 和 $W_{K}$ 中，我们可以将公式（1）重写为： 

![image-20241121162449513](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211624594.png)

其中， $Q_{n}(\gamma e^{i\theta})^{n}$、 $K_{m}(\gamma e^{i\theta})^{-m}$ 被称为 $xPos $ [SDP + 22]，也就是为Transformer提出的一种相对位置嵌入。我们进一步将 $\gamma $ 简化为一个标量，此时公式（3）变为： 

![image-20241121163648949](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211636999.png)

其中，$\dagger$ 表示共轭转置。该表达式在训练实例中很容易实现并行化。 总而言之，我们从公式（1）所示的循环建模入手，然后推导出了公式（4）中的并行表达式。我们将原始映射$v(n) \mapsto o(n)$ 视为向量，并得出如下的保留机制。 

**记忆力机制的并行表示**

![image-20241121164137508](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211641621.png)

> 保留网络（RetNet）的双重形式。“GN”是“组归一化（GroupNorm）”的缩写。

如图3a所示，记忆层（Retention layer）被定义为：

![image-20241121164902331](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211649390.png)

其中，$\bar{\Theta}$ 是 $\Theta$ 的复共轭，并且 $D \in \mathbb{R}^{|x| \times |x|}$ 将因果掩码（causal masking）以及沿相对距离的指数衰减合并为一个矩阵。与自注意力机制类似，并行表示使我们能够利用GPU高效地训练模型。 

**记忆力机制的循环表示** 

如图3b所示，所提出的机制也可以写成循环神经网络（RNNs）的形式，这对推理是有利的。对于第\(n\)个时间步，我们通过循环的方式来获取输出，其计算方式如下：

![image-20241121182146243](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211821330.png)

其中，$Q$、$K$、$V$、$\gamma$ 与公式（5）中的相同。

**记忆力机制的分块循环表示**

存在一种并行表示与循环表示相混合的形式，可用于加速训练，尤其适用于长序列情况。我们将输入序列划分为多个块。在每个块内，我们依照并行表示（公式（5））来进行计算。相比之下，跨块信息则按照循环表示（公式（6））来传递。具体而言，设\(B\)表示块的长度。我们通过以下方式来计算第 $i$ 个块的记忆力输出： 

![image-20241121182638966](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211826048.png)

其中，$[i]$ 表示第 $i$ 个块，即$x_{[i]} = [x_{(i - 1)B + 1}, \cdots, x_{iB}]$。=

### 2.2 门控多尺度记忆力

我们在每一层使用 $h = d_{model} / d$ 个记忆力头（retention heads），其中 $d$ 是头维度。这些头使用不同的参数矩阵$W_{Q}$、$W_{K}$、$W_{V} \in \mathbb{R}^{d \times d}$。此外，多尺度记忆力（MSR）机制为每个头分配不同的 $\gamma$ 值。为简便起见，我们设定不同层之间的 $\gamma$ 值相同，并使其保持固定。另外，我们添加了一个Swish门函数（[HG16, RZL17]）以增加保留层的非线性特性。形式上，给定输入 $X$，我们将该层定义为： 

![image-20241121183218587](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211832698.png)

其中，$W_{G}$、$W_{O} \in \mathbb{R}^{d_{model} \times d_{model}}$是可学习的参数，并且组归一化（GroupNorm）[WH18]会按照[SPP + 19]中提出的子层归一化（SubLN）对每个头的输出进行归一化处理。需要注意的是，由于这些头使用了多个 $\gamma$尺度，这会导致不同的 方差统计情况。所以我们对各个头的输出分别进行归一化操作。 记忆力机制的伪代码总结在图4中。 

**记忆力分数归一化** 

我们利用组归一化（GroupNorm）的尺度不变性来提高记忆力层的数值精度。具体而言，在组归一化中乘以一个标量值不会影响输出以及反向梯度，即组归一化（$\alpha * \text{head}_{i}$） = 组归一化（$\text{head}_{i}$）。 我们在公式（5）中实现了三个归一化因子。首先，我们将 $QK^{\top}$ 进行归一化，处理为 $QK^{\top} / \sqrt{d}$。其次，我们用 $\widetilde{D}_{nm} = D_{nm} / \sqrt{\sum_{i = 1}^{n} D_{ni}}$ 来替换 $D$。第三，令 $R$ 表示记忆力分数，即 $R = QK^{\top} \odot D$，我们将其归一化处理为 $\widetilde{R}_{nm} = R_{nm} / \max(\left|\sum_{i = 1}^{n} R_{ni}\right|, 1)$。那么记忆力输出就变为 $\text{Retention}(X) = \widetilde{R}V$。 由于尺度不变性，上述技巧在稳定前向和反向传播的数值流的同时，并不会影响最终结果。 

### 2.3 记忆力网络的整体架构

对于一个具有 $L$ 层的保留网络（retention network），我们堆叠多尺度保留（multi-scale retention，简称MSR）模块和前馈网络（feed-forward network，简称FFN）来构建模型。形式上，输入序列 $\{x_{i}\}_{i = 1}^{|x|}$ 会先通过一个词嵌入层被转换为向量。我们将打包后的嵌入向量 $X^{0} = [x_{1}, \cdots, x_{|x|}] \in \mathbb{R}^{|x| \times d_{model}}$ 作为输入，并计算模型的输出 $X^{L}$： 

![image-20241121190641462](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211906525.png)

其中，$LN(\cdot)$ 是层归一化（LayerNorm）[BKH16]。前馈网络（FFN）部分的计算方式为 $FFN(X) = \text{gelu}(XW_{1})W_{2}$，其中 $W_{1}$、$W_{2}$ 是参数矩阵。 

**训练** 我们在训练过程中使用并行表示（公式（5））以及分块循环表示（公式（7））。序列内或块内的并行化能够高效利用GPU来加速计算。更有利的是，分块循环表示对于长序列训练尤为有用，它在浮点运算次数（FLOPs）和内存消耗方面都很高效。 

**推理** 在推理过程中采用循环表示（公式（6）），它与自回归解码（autoregressive decoding）契合得很好。其具有\(O(1)\)复杂度，在实现等效结果的同时，能够减少内存占用以及推理延迟。 

### 2.4 与先前方法的关联及差异

![image-20241121191039541](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411211910643.png)

> 表1：从多个角度进行的模型比较。保留网络（RetNet）实现了训练并行化、恒定的推理成本、针对长序列的线性内存复杂度以及良好的性能表现。

表1从多个角度对记忆网络（RetNet）与先前的方法进行了比较。比较结果与图2中呈现的“不可能三角”相呼应。此外，由于采用了分块循环表示，记忆网络对于长序列具有线性的内存复杂度。我们还将其与特定方法的比较情况总结如下： 

**Transformer** 记忆力机制的并行表示与Transformer[VSP + 17]有着相似的思路。与之最相关的Transformer变体是Lex Transformer[SDP + 22]，它将xPos用作位置嵌入。如公式（3）所述，记忆力机制的推导与xPos是一致的。与注意力机制相比，记忆力机制去掉了softmax并且能够以循环形式来表示，这对推理有显著的益处。 

**S4** 与公式（2）不同的是，如果 $Q_{n}$ 和 $K_{n}$ 与内容无关，该表达式可退化为S4[GGR21]，其中 $O=(QK^{\top}, QAK^{\top}, \cdots, QA^{|x| - 1}K^{\top}) * V$。 

**线性注意力** 其变体通常使用各种各样的核函数 $\phi(q_{i})\phi(k_{j}) / \sum_{n = 1}^{|x|} \phi(q_{i})\phi(k_{n})$ 来替代softmax函数。然而，线性注意力很难有效地对位置信息进行编码，这使得模型的性能欠佳。此外，我们是从头重新审视序列建模，而不是旨在去逼近softmax。 

**AFT/RWKV** 无注意力Transformer（Attention Free Transformer，简称AFT）将点积注意力简化为逐元素运算，并将softmax移到了键向量上。RWKV用指数衰减取代了AFT的位置嵌入，并以循环方式运行模型来进行训练和推理。相比之下，保留机制保留了高维状态来对序列信息进行编码，这有助于提升表达能力以及获得更好的性能。 

**xPos/RoPE** 与为Transformer提出的相对位置嵌入方法相比，公式（3）呈现出与xPos[SDP + 22]以及旋转位置嵌入（RoPE）[SLP + 21]类似的表达式。 

**Sub - LayerNorm** 如公式（8）所示，保留层使用子层归一化（Sub - LayerNorm）[WMH + 22]对输出进行归一化。由于多尺度建模会导致各个头具有不同的方差，我们用组归一化（GroupNorm）取代了原来的层归一化（LayerNorm）。 



## 三、实验

我们开展了语言建模实验来评估保留网络（RetNet）。我们使用各种基准测试来评估所提出的架构，也就是语言建模性能以及在下游任务中的零次学习和少次学习情况。此外，在训练和推理方面，我们对速度、内存消耗以及延迟进行了比较。

### 3.1 设置

![image-20241128131330251](https://raw.githubusercontent.com/Quinlan7/pic_cloud/main/img/202411281313473.png)

**参数分配** 为了进行公平比较，我们对多尺度保留（MSR）和前馈网络（FFN）中的参数进行了重新分配。在此为简便起见，令 $d$ 表示 $d_{model}$。在Transformer中，自注意力机制大约有 $4d^{2}$ 个参数（其中 $W_{Q}$ 、 $W_{K}$、$W_{V}$、$W_{O} \in \mathbb{R}^{d \times d}$），前馈网络中有 $8d^{2}$ 个参数（其中中间维度为$4d$）。相比之下，保留网络（RetNet）在保留层中有 $8d^{2}$ 个参数（其中 $W_{Q}$、$W_{K} \in \mathbb{R}^{d \times d}$，$W_{G}$、$W_{V} \in \mathbb{R}^{d \times 2d}$，$W_{O} \in \mathbb{R}^{2d \times d}$）。需要注意的是，$V$ 的头维度是 $Q$、$K$ 的两倍，通过 $W_{O}$ 将扩展的维度投影回 $d$ 。为了使参数数量与Transformer保持相同，保留网络中前馈网络的中间维度设为 $2d$。同时，在我们的实验中，将头维度设为\(256\)，即查询（queries）和键（keys）的维度为\(256\)，值（values）的维度为\(512\)。为了公平比较，我们在不同模型规模下保持 $\gamma$ 相同，$\gamma = 1 - e^{\text{linspace}(\log \frac{1}{32}, \log \frac{1}{512}, h)} \in \mathbb{R}^{h}$，而非公式（8）中的默认值。 

**语言模型训练** 如表2所示，我们从头开始训练不同规模（即13亿、27亿和67亿参数）的语言模型。训练语料是经过精心整理的《堆集》（The Pile）[GBB + 20]、C4 [DMI + 21]以及《栈集》（The Stack）[KLBA + 22]的合集。我们添加了`<bos>`标记来表示序列的开始。训练批次大小为400万个词元（tokens），最大长度为2048。我们使用1000亿个词元来训练模型，也就是25000个训练步骤。我们使用AdamW [LH19]优化器，其中$\beta_{1} = 0.9$，$\beta_{2} = 0.98$，权重衰减设置为 $0.05$。热身步骤（warmup steps）的数量为375，学习率采用线性衰减方式。参数按照DeepNet [WMD + 22]进行初始化以保证训练的稳定性。实现基于TorchScale [MWH + 22]，我们使用512个AMD MI200 GPU来训练模型。 